{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## I. Beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SESSION_USER = 'agalashov'\n",
    "IS_REUSE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Include libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T01:30:51.103891",
     "start_time": "2017-04-18T01:30:45.346328"
    },
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# ---\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "# import seaborn as sns\n",
    "# sns.set(color_codes=True)\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Performing grid search\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Feature Importance\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# I/O tools\n",
    "from lib import io_tools\n",
    "from lib import preprocessing_tools\n",
    "from lib import analysis_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calculate_global_freq(dataframe, col_name):\n",
    "    col_global_freq_name = str(col_name) + \"_global_freq\"\n",
    "    col_global_flow_freq_name = str(col_name) + \"_global_freq_flow\"\n",
    "    col_global_noflow_freq_name = str(col_name) + \"_global_freq_noflow\"\n",
    "    col_global_listened_freq_name = str(col_name) + \"_global_freq_listened\"\n",
    "    col_global_flow_listened_freq_name = str(col_name) + \"_global_freq_flow_listened\"    \n",
    "    col_global_noflow_listened_freq_name = str(col_name) + \"_global_freq_nopflow_listened\"        \n",
    "    \n",
    "    cols = [col_name,\n",
    "            col_global_freq_name,\n",
    "            col_global_flow_freq_name,\n",
    "            col_global_noflow_freq_name,\n",
    "            col_global_listened_freq_name,\n",
    "            col_global_flow_listened_freq_name,\n",
    "            col_global_noflow_listened_freq_name]\n",
    "    \n",
    "    training_size = len(dataframe.index)\n",
    "    \n",
    "    print 'Executing calculations for ', col_name\n",
    "    print 'Extracting listened type information'\n",
    "    \n",
    "    dataframe_flow = dataframe[dataframe['listen_type'] == 1]\n",
    "    dataframe_noflow = dataframe[dataframe['listen_type'] == 0]\n",
    "    dataframe_listened = dataframe[dataframe['is_listened'] == 1]\n",
    "    dataframe_listened_flow = dataframe_listened[dataframe_listened['listen_type'] == 1]\n",
    "    dataframe_listened_noflow = dataframe_listened[dataframe_listened['listen_type'] == 0]\n",
    "    \n",
    "    col_global_freq = dataframe[col_name].value_counts() / training_size\n",
    "    col_global_flow_freq = dataframe_flow[col_name].value_counts() / training_size\n",
    "    col_global_noflow_freq = dataframe_noflow[col_name].value_counts() / training_size\n",
    "    col_global_listened_freq = dataframe_listened[col_name].value_counts() / training_size\n",
    "    col_global_flow_listened_freq = dataframe_listened_flow[col_name].value_counts() / training_size\n",
    "    col_global_noflow_listened_freq = dataframe_listened_noflow[col_name].value_counts() / training_size\n",
    "    \n",
    "    columns_dict = {col_global_freq_name : col_global_freq,\n",
    "                    col_global_flow_freq_name: col_global_flow_freq,\n",
    "                    col_global_noflow_freq_name: col_global_noflow_freq, \n",
    "                    col_global_listened_freq_name: col_global_listened_freq,\n",
    "                    col_global_flow_listened_freq_name: col_global_flow_listened_freq,\n",
    "                    col_global_noflow_listened_freq_name:  col_global_noflow_listened_freq\n",
    "                   }\n",
    "    result_dataframe = pd.DataFrame(columns_dict).reset_index().rename(columns={'index' : col_name}).fillna(value=0)\n",
    "        \n",
    "    return  result_dataframe\n",
    "def add_global_freq(dataframe, global_freq_columns):\n",
    "    result_dataframe = dataframe\n",
    "    for col_name in global_freq_columns:        \n",
    "        result_dataframe = pd.merge(result_dataframe, calculate_global_freq(result_dataframe, col_name), on=[col_name], how='left')\n",
    "    return result_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calculate_user_level_column(dataframe, col_name):\n",
    "    user_ids = dataframe['user_id'].unique()\n",
    "    col_freq_name = col_name + '_user_freq'\n",
    "    col_freq_flow_name = col_name + '_user_freq_flow'\n",
    "    col_freq_noflow_name = col_name + '_user_freq_noflow'\n",
    "    col_freq_listened_name = col_name + '_user_freq_listened'\n",
    "    col_freq_flow_listened_name = col_name + '_user_freq_flow_listened'\n",
    "    col_freq_noflow_listened_name = col_name + '_user_freq_noflow_listened'\n",
    "    cols = ['user_id', col_name, col_freq_name, col_freq_flow_name,\n",
    "            col_freq_noflow_name, col_freq_listened_name, col_freq_flow_listened_name,\n",
    "            col_freq_noflow_listened_name]\n",
    "    result_dataframe = pd.DataFrame(columns=cols)\n",
    "    print 'calculations for ', col_name, 'begin...'\n",
    "    counter = 0\n",
    "    for user_id in user_ids:\n",
    "        dataframe_user = dataframe[dataframe['user_id'] == user_id]\n",
    "        \n",
    "        training_size = len(dataframe_user.index)\n",
    "        \n",
    "        dataframe_user_flow = dataframe_user[dataframe_user['listen_type'] == 1]\n",
    "        dataframe_user_noflow = dataframe_user[dataframe_user['listen_type'] == 0]\n",
    "        dataframe_user_listened = dataframe_user[dataframe_user['is_listened'] == 1]\n",
    "        \n",
    "        dataframe_user_flow_listened = dataframe_user_flow[dataframe_user_flow['is_listened'] == 1]\n",
    "        dataframe_user_noflow_listened = dataframe_user_noflow[dataframe_user_noflow['is_listened'] == 1]\n",
    "        \n",
    "        col_freq_value = dataframe_user[col_name].value_counts() / training_size\n",
    "        \n",
    "        col_freq_flow_value = dataframe_user_flow[col_name].value_counts() / training_size\n",
    "        \n",
    "        col_freq_noflow_value = dataframe_user_noflow[col_name].value_counts() / training_size\n",
    "        \n",
    "        col_freq_listened_value = dataframe_user_listened[col_name].value_counts() / training_size\n",
    "\n",
    "        col_freq_flow_listened_value = dataframe_user_flow_listened[col_name].value_counts() / training_size\n",
    "        \n",
    "        col_freq_noflow_listened_value = dataframe_user_noflow_listened[col_name].value_counts() / training_size\n",
    "        \n",
    "        columns_dict = {\n",
    "            col_freq_name : col_freq_value,\n",
    "            col_freq_flow_name : col_freq_flow_value,\n",
    "            col_freq_noflow_name : col_freq_noflow_value,\n",
    "            col_freq_listened_name : col_freq_listened_value,\n",
    "            col_freq_flow_listened_name : col_freq_flow_listened_value,\n",
    "            col_freq_noflow_listened_name : col_freq_noflow_listened_value,\n",
    "        }\n",
    "        \n",
    "        df2 = pd.DataFrame(columns_dict).reset_index().rename(columns={'index' : col_name}).fillna(value=0)\n",
    "        \n",
    "#         df2 = pd.DataFrame([])\n",
    "        \n",
    "#         df2 = col_value_for_user.reset_index(name=col_freq_name).rename(columns={'index' : col_name})\n",
    "        df2['user_id'] = user_id\n",
    "        result_dataframe = result_dataframe.append(df2[cols])    \n",
    "        counter+= 1\n",
    "        \n",
    "        if counter % 1000 == 0:\n",
    "            print counter, user_id\n",
    "    return result_dataframe\n",
    "def add_user_level_frequencies(dataframe, user_frequency_columns):\n",
    "    result_dataframe = dataframe\n",
    "    for col_name in user_frequency_columns:\n",
    "        result_dataframe = pd.merge(result_dataframe, calculate_user_level_column(result_dataframe, col_name), on=['user_id', col_name], how='left')\n",
    "    return result_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def add_categorical_variables(dataframe, categorical_variables_list):\n",
    "    def dummify(dataframe, categorical_variable):\n",
    "        local_dummies = pd.get_dummies(dataframe[categorical_variable],prefix=categorical_variable)\n",
    "        dataframe = pd.concat([dataframe,local_dummies],axis=1)\n",
    "        return dataframe.drop(categorical_variable,axis=1)\n",
    "    \n",
    "    for categorical_variable in categorical_variables_list:\n",
    "        dataframe = dummify(dataframe, categorical_variable)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-04-17T23:31:22.494Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def aggregation_functions(dataframe, train_dataframe, name):\n",
    "    combined = merged_datasets\n",
    "    train = preprocessed_train_dataset\n",
    "\n",
    "    avrg = train.groupby([name + '_id'])['is_listened'].mean()\n",
    "    avrg_flow = train[train['listen_type'] == 1].groupby([name + '_id'])['is_listened'].mean()\n",
    "    avrg_noflow = train[train['listen_type'] == 0].groupby([name + '_id'])['is_listened'].mean()\n",
    "\n",
    "    avrg.name = 'avrg_listened_' + name\n",
    "    avrg_flow.name = 'avrg_listened_' + name + '_flow'\n",
    "    avrg_noflow.name = 'avrg_listened_' + name + '_noflow'\n",
    "\n",
    "    combined = combined.join(avrg, name + '_id', 'left')\n",
    "    combined = combined.join(avrg_flow, name + '_id', 'left')\n",
    "    combined = combined.join(avrg_noflow, name + '_id', 'left')\n",
    "\n",
    "    combined['avrg_listened_' + name].fillna(np.median(combined[~np.isnan(combined['avrg_listened_' + name])]['avrg_listened_' + name]), inplace=True)\n",
    "\n",
    "    combined['avrg_listened_' + name + '_flow'].fillna(combined['avrg_listened_' + name], inplace=True)\n",
    "    combined['avrg_listened_' + name + '_noflow'].fillna(combined['avrg_listened_' + name], inplace=True)\n",
    "\n",
    "    combined['avrg_listened_' + name + '_current'] = combined['avrg_listened_' + name + '_flow']*combined['listen_type'] + combined['avrg_listened_' + name + '_noflow']*(1-combined['listen_type'])\n",
    "\n",
    "    count_ = combined.groupby([name + '_id'])[name + '_id'].count()\n",
    "    count_.name = 'count_' + name\n",
    "    combined = combined.join(count_, name + '_id', 'left')\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transform_dates(dataframe):\n",
    "    conv_release_date_to_year = lambda x: datetime.datetime.strptime(str(x), '%Y%m%d').strftime('%Y')\n",
    "    conv_timestamp_to_hour = lambda x: datetime.datetime.utcfromtimestamp(x).hour\n",
    "    conv_timestamp_to_weekday = lambda x: datetime.datetime.utcfromtimestamp(x).weekday()\n",
    "    dataframe['release_year'] = dataframe['release_date'].apply(conv_release_date_to_year)\n",
    "    dataframe['timestamp_hour'] = dataframe['ts_listen'].apply(conv_timestamp_to_hour)\n",
    "    dataframe['timestamp_weekday'] = dataframe['ts_listen'].apply(conv_timestamp_to_weekday)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def add_time_moments(dataframe):\n",
    "    dataframe['is_morning'] = ((dataframe['timestamp_hour'] >= 6) & (dataframe['timestamp_hour'] < 12)).astype(int)\n",
    "    dataframe['is_day'] = ((dataframe['timestamp_hour'] >= 12) & (dataframe['timestamp_hour'] < 18)).astype(int)\n",
    "    dataframe['is_evening'] = ((dataframe['timestamp_hour'] >= 18) & (dataframe['timestamp_hour'] < 22)).astype(int)\n",
    "    dataframe['is_night'] = ((dataframe['timestamp_hour'] >= 22) | (dataframe['timestamp_hour'] < 6)).astype(int)                                                                 \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def transform_userage(dataframe):\n",
    "    dataframe['18-21'] = ((dataframe['user_age'] >= 18) & (dataframe['user_age'] <= 21)).astype(int)\n",
    "    dataframe['22-25'] = ((dataframe['user_age'] >= 22) & (dataframe['user_age'] <= 25)).astype(int)\n",
    "    dataframe['26-30'] = ((dataframe['user_age'] >= 26) & (dataframe['user_age'] <= 30)).astype(int)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def add_contex_type(dataframe):\n",
    "    dataframe['context_type_1'] = (dataframe['context_type'] == 1).astype(int)\n",
    "    dataframe['context_type_5'] = (dataframe['context_type'] == 5).astype(int)\n",
    "    dataframe['context_type_20'] = (dataframe['context_type'] == 20).astype(int)\n",
    "    dataframe['context_type_23'] = (dataframe['context_type'] == 23).astype(int)\n",
    "    dataframe['context_type_ot'] = ((dataframe['context_type'] != 1) & (dataframe['context_type'] != 5) &\n",
    "                                    (dataframe['context_type'] != 20) & (dataframe['context_type'] != 23)).astype(int)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_features_to_test_set(train_set, test_set):\n",
    "    columns = ['media_id', 'album_id', 'artist_id', 'genre_id']\n",
    "    global_columns = []\n",
    "    local_columns = ['user_id']\n",
    "    for col in columns:    \n",
    "        local_columns.append(col + '_user_freq')\n",
    "        local_columns.append(col + '_user_freq_flow')\n",
    "        local_columns.append(col + '_user_freq_noflow')\n",
    "        local_columns.append(col + '_user_freq_listened')\n",
    "        local_columns.append(col + '_user_freq_flow_listened')\n",
    "        local_columns.append(col + '_user_freq_noflow_listened')    \n",
    "        local_columns.append(col)\n",
    "\n",
    "        global_columns.append(col)\n",
    "        global_columns.append(col + '_global_freq')\n",
    "        global_columns.append(col + '_global_freq_flow')\n",
    "        global_columns.append(col + '_global_freq_noflow')\n",
    "        global_columns.append(col + '_global_freq_listened')\n",
    "        global_columns.append(col + '_global_freq_flow_listened')\n",
    "        global_columns.append(col + '_global_freq_nopflow_listened')\n",
    "        \n",
    "    test_set = pd.merge(test_set, train_set[global_columns].drop_duplicates(), on=columns, how='left')    \n",
    "    columns.append('user_id')\n",
    "    test_set = pd.merge(test_set, train_set[local_columns].drop_duplicates(), on=columns, how='left')\n",
    "    test_set = test_set.fillna(value=0)\n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T01:30:51.533017",
     "start_time": "2017-04-18T01:30:51.105804"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T01:31:01.733376",
     "start_time": "2017-04-18T01:30:51.534240"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning features calculation\n",
      "For column= media_id  train has  452975  and test has  9732  values\n",
      "For column= genre_id  train has  2922  and test has  455  values\n",
      "For column= album_id  train has  151471  and test has  7015  values\n",
      "For column= user_id  train has  19918  and test has  19918  values\n",
      "For column= artist_id  train has  67142  and test has  3950  values\n",
      "17000 19120\n",
      "18000 14268\n",
      "19000 19202\n",
      "5000 4254\n",
      "6000 1616\n",
      "7000 8151\n",
      "8000 11632\n",
      "9000 12992\n",
      "10000 15942\n",
      "11000 17694\n",
      "12000 12283\n"
     ]
    }
   ],
   "source": [
    "if IS_REUSE == False:\n",
    "    print 'Beginning features calculation'\n",
    "    #LOAD DATA\n",
    "    raw_train_dataset, raw_test_dataset, SAMPLE_ID_TEST = io_tools.read_data(train_filename='./data/train.csv',\n",
    "                                                         test_filename='./data/test.csv')\n",
    "    columns_to_calculacte = ['media_id', 'genre_id', 'album_id', 'user_id', 'artist_id']\n",
    "    for col in columns_to_calculacte:\n",
    "        print 'For column=', col,' train has ', len(raw_train_dataset[col].unique()), ' and test has ', len(raw_test_dataset[col].unique()),' values'\n",
    "        \n",
    "    preprocessed_train_dataset, preprocessed_test_dataset = preprocessing_tools.preprocess_data(raw_train_dataset, raw_test_dataset)    \n",
    "    \n",
    "    # Transform data formats\n",
    "    preprocessed_train_dataset = transform_dates(preprocessed_train_dataset)\n",
    "    preprocessed_test_dataset = transform_dates(preprocessed_test_dataset)\n",
    "    \n",
    "    # Add time moments\n",
    "    preprocessed_train_dataset = add_time_moments(preprocessed_train_dataset)\n",
    "    preprocessed_test_dataset = add_time_moments(preprocessed_test_dataset)\n",
    "    \n",
    "    # Transform userage\n",
    "    preprocessed_train_dataset = transform_userage(preprocessed_train_dataset)\n",
    "    preprocessed_test_dataset = transform_userage(preprocessed_test_dataset)\n",
    "    \n",
    "    # Add context type\n",
    "    preprocessed_train_dataset = add_contex_type(preprocessed_train_dataset)\n",
    "    preprocessed_test_dataset = add_contex_type(preprocessed_test_dataset)\n",
    "    \n",
    "    columns = ['media_id', 'album_id', 'artist_id', 'genre_id']\n",
    "    preprocessed_train_dataset = add_global_freq(preprocessed_train_dataset, columns)\n",
    "    preprocessed_train_dataset = add_user_level_frequencies(preprocessed_train_dataset, columns)\n",
    "    \n",
    "    preprocessed_test_dataset = add_features_to_test_set(preprocessed_train_dataset, preprocessed_test_dataset)\n",
    "    \n",
    "    date_str = datetime.datetime.now().strftime(\"%d_%m_%y_%H_%M\")\n",
    "    train_set_name = str(date_str) + \"_\"+ SESSION_USER + \"_train.csv\"\n",
    "    test_set_name = str(date_str) + \"_\"+ SESSION_USER + \"_test.csv\"\n",
    "    \n",
    "    preprocessed_train_dataset.to_csv('./preprocessed_features/'+str(train_set_name))\n",
    "    preprocessed_test_dataset.to_csv('./preprocessed_features/'+str(test_set_name))\n",
    "    del raw_train_dataset\n",
    "    del raw_test_dataset\n",
    "else:\n",
    "    print 'Reusing precalculated features'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'da'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocessed_train_dataset = pd.read_csv('important_preprocessed_train.csv')\n",
    "preprocessed_train_dataset.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "raw_train_dataset, raw_test_dataset, SAMPLE_ID_TEST = io_tools.read_data(train_filename='./data/train.csv',\n",
    "                                                         test_filename='./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "columns = ['media_id', 'album_id', 'artist_id', 'genre_id']\n",
    "global_columns = []\n",
    "local_columns = ['user_id']\n",
    "for col in columns:    \n",
    "    local_columns.append(col + '_user_freq')\n",
    "    local_columns.append(col + '_user_freq_flow')\n",
    "    local_columns.append(col + '_user_freq_noflow')\n",
    "    local_columns.append(col + '_user_freq_listened')\n",
    "    local_columns.append(col + '_user_freq_flow_listened')\n",
    "    local_columns.append(col + '_user_freq_noflow_listened')    \n",
    "    local_columns.append(col)\n",
    "    \n",
    "    global_columns.append(col)\n",
    "    global_columns.append(col + '_global_freq')\n",
    "    global_columns.append(col + '_global_freq_flow')\n",
    "    global_columns.append(col + '_global_freq_noflow')\n",
    "    global_columns.append(col + '_global_freq_listened')\n",
    "    global_columns.append(col + '_global_freq_flow_listened')\n",
    "    global_columns.append(col + '_global_freq_nopflow_listened')\n",
    "    \n",
    "preprocessed_test_dataset = pd.merge(preprocessed_test_dataset, preprocessed_train_dataset[columns_to_extract].drop_duplicates(), on=columns, how='left')    \n",
    "columns.append('user_id')\n",
    "preprocessed_test_dataset = pd.merge(preprocessed_test_dataset, preprocessed_train_dataset[local_columns].drop_duplicates(), on=columns, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocessed_test_dataset = preprocessed_test_dataset.fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocessed_test_dataset.to_csv('./important_preprocessed_testset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del raw_train_dataset\n",
    "del raw_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_before(dataframe):\n",
    "    dataframe.drop(['genre_id', 'ts_listen', 'media_id',\n",
    "                   'album_id', 'release_date', 'user_id', 'artist_id', 'release_date'], axis=1, inplace=True)\n",
    "    dataframe.drop(['release_year', 'timestamp_weekday'], axis=1, inplace=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocessed_train_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "col_name = 'media_id'\n",
    "print preprocessed_train_dataset[[col_name + str('_user_freq'), 'is_listened']].corr()\n",
    "print preprocessed_train_dataset[[col_name + str('_user_freq_flow'), 'is_listened']].corr()\n",
    "print preprocessed_train_dataset[[col_name + str('_user_freq_noflow'), 'is_listened']].corr()\n",
    "print preprocessed_train_dataset[[col_name + str('_user_freq_listened'), 'is_listened']].corr()\n",
    "print preprocessed_train_dataset[[col_name + str('_user_freq_flow_listened'), 'is_listened']].corr()\n",
    "print preprocessed_train_dataset[[col_name + str('_user_freq_noflow_listened'), 'is_listened']].corr()\n",
    "\n",
    "print preprocessed_train_dataset[[col_name + str('_global_freq'), 'is_listened']].corr()\n",
    "print preprocessed_train_dataset[[col_name + str('_global_freq_flow'), 'is_listened']].corr()\n",
    "print preprocessed_train_dataset[[col_name + str('_global_freq_noflow'), 'is_listened']].corr()\n",
    "print preprocessed_train_dataset[[col_name + str('_global_freq_listened'), 'is_listened']].corr()\n",
    "print preprocessed_train_dataset[[col_name + str('_global_freq_flow_listened'), 'is_listened']].corr()\n",
    "print preprocessed_train_dataset[[col_name + str('_global_noflow_listened'), 'is_listened']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def postprocess(dataframe):\n",
    "    dataframe.drop(['genre_id', 'ts_listen', 'media_id',\n",
    "                   'album_id', 'release_date', 'user_id', 'artist_id', 'release_date'], axis=1, inplace=True)\n",
    "    dataframe.drop(['release_year', 'timestamp_weekday'], axis=1, inplace=True)\n",
    "    dataframe = add_time_moments(dataframe)\n",
    "    dataframe.drop(['timestamp_hour'], axis=1, inplace=True)\n",
    "    dataframe = transform_userage(dataframe)\n",
    "    dataframe.drop(['user_age'], axis=1, inplace=True)\n",
    "    dataframe = add_contex_type(dataframe)\n",
    "    dataframe.drop(['context_type'], axis=1, inplace=True)    \n",
    "#     dataframe = pd.get_dummies(dataframe, columns=['platform_name', 'platform_family'])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# preprocessed_test_dataset = postprocess(preprocessed_test_dataset)\n",
    "# preprocessed_test_dataset = pd.get_dummies(preprocessed_test_dataset, columns=['platform_name'])\n",
    "# preprocessed_test_dataset = pd.get_dummies(preprocessed_test_dataset, columns=['platform_family'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# preprocessed_test_dataset.to_csv('postprocessed_new_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# preprocessed_train_dataset = postprocess(preprocessed_train_dataset)\n",
    "# preprocessed_train_dataset = pd.get_dummies(preprocessed_train_dataset, columns=['platform_name'])\n",
    "# preprocessed_train_dataset = pd.get_dummies(preprocessed_train_dataset, columns=['platform_family'])\n",
    "# preprocessed_test_dataset = postprocess(preprocessed_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# preprocessed_train_dataset.to_csv('postprocessed_new_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(preprocessed_train_dataset, columns=['platform_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del preprocessed_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocessed_test_dataset = postprocess(preprocessed_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocessed_train_dataset = preprocessed_train_dataset.fillna(value=0)\n",
    "preprocessed_test_dataset = preprocessed_test_dataset.fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# preprocessed_train_dataset.to_csv('postprocessed_new_train.csv')\n",
    "# preprocessed_test_dataset.to_csv('postprocessed_new_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del preprocessed_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_labels = preprocessed_train_dataset['is_listened'].as_matrix()\n",
    "# preprocessed_train_dataset.drop(['is_listened'], axis=1, inplace=True)\n",
    "train_set = preprocessed_train_dataset.as_matrix()\n",
    "del preprocessed_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "amount_of_data = 1.0\n",
    "n = np.shape(train_set)[0]\n",
    "permutation = np.array(range(n))\n",
    "np.random.shuffle(permutation)\n",
    "part_of_data = train_set[:int(n * amount_of_data), :]\n",
    "part_of_labels = train_labels[:int(n * amount_of_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def crossvalidate_me(data, labels, depths, estimators, pca_components):\n",
    "    n_folds = 3\n",
    "    for depth in depths:\n",
    "        for n_estimators in estimators:\n",
    "            for n_pca in pca_components:\n",
    "                print depth, n_estimators, n_pca\n",
    "                kf = KFold(n_splits=n_folds)\n",
    "                final_accuracy = 0\n",
    "\n",
    "                for train, test in kf.split(data):\n",
    "                    training_data, training_labels = data[train], labels[train]\n",
    "                    test_data, test_labels = data[test], labels[test] \n",
    "                \n",
    "                    pca = PCA(n_components=n_pca)\n",
    "                    train_pca = pca.fit_transform(training_data)\n",
    "                    scaler = StandardScaler()\n",
    "                    train_scaled = scaler.fit_transform(train_pca)\n",
    "                    \n",
    "                    xgb = XGBClassifier(max_depth = depth, n_estimators=n_estimators)\n",
    "    \n",
    "                    print 'start training'\n",
    "                    xgb.fit(train_scaled, training_labels)\n",
    "\n",
    "                    test_pca = pca.transform(test_data)\n",
    "                    test_pca_scaled = scaler.transform(test_pca)        \n",
    "\n",
    "                    current_acc = accuracy_score(test_labels, xgb.predict(test_pca_scaled))\n",
    "\n",
    "                    final_accuracy += current_acc\n",
    "\n",
    "                    print 'done', current_acc\n",
    "                print depth, n_estimators, n_pca, final_accuracy / n_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "crossvalidate_me(part_of_data, part_of_labels, [3,4,5], estimators=[80,100], pca_components=[50,40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "kf = KFold(n_splits=5)\n",
    "final_accuracy = 0\n",
    "for train, test in kf.split(part_of_data):\n",
    "    training_data, training_labels = part_of_data[train], part_of_labels[train]\n",
    "    test_data, test_labels = part_of_data[test], part_of_labels[test] \n",
    "    pca = PCA(n_components=50)\n",
    "    train_pca = pca.fit_transform(training_data)\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train_pca)\n",
    "    \n",
    "    xgb = XGBClassifier(max_depth = 4, n_estimators=100)\n",
    "    \n",
    "    print 'start training'\n",
    "    \n",
    "    xgb.fit(train_scaled, training_labels)\n",
    "\n",
    "    test_pca = pca.transform(test_data)\n",
    "    test_pca_scaled = scaler.transform(test_pca)        \n",
    "    \n",
    "    current_acc = accuracy_score(test_labels, xgb.predict(test_pca_scaled))\n",
    "    \n",
    "    final_accuracy += current_acc\n",
    "    \n",
    "    print 'done', current_acc\n",
    "    \n",
    "final_accuracy /= 5\n",
    "print final_accuracy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('postprocessed_new_train.csv')\n",
    "# train_labels = train_set['is_listened'].as_matrix()\n",
    "# train_set.drop(['is_listened', 'Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['media_id_global_freq_flow',\n",
    "                    'media_id_global_freq',\n",
    "                    'media_id_global_freq_noflow',\n",
    "                    'media_id_user_freq_flow', \n",
    "                    'media_id_user_freq_noflow',  \n",
    "                   \n",
    "                    'genre_id_global_freq',\n",
    "                    'genre_id_global_freq_flow',\n",
    "                    'genre_id_global_freq_flow_listened',\n",
    "                    'genre_id_global_freq_listened',\n",
    "                    'genre_id_global_freq_noflow',\n",
    "                    'genre_id_global_freq_nopflow_listened',                    \n",
    "                    'genre_id_user_freq_flow', \n",
    "                    'genre_id_user_freq_noflow',\n",
    "                    \n",
    "                    'artist_id_global_freq_flow',\n",
    "                    'artist_id_global_freq',\n",
    "                    'artist_id_global_freq_noflow',\n",
    "                    'artist_id_user_freq_flow', \n",
    "                    'artist_id_user_freq_noflow',\n",
    "                    \n",
    "                    'album_id_global_freq_flow',\n",
    "                    'album_id_global_freq',\n",
    "                    'album_id_global_freq_noflow',\n",
    "                    'album_id_user_freq_flow', \n",
    "                    'album_id_user_freq_noflow',\n",
    "                   ]\n",
    "#'genre_id', \n",
    "# colnames = ['media_id', 'album_id', 'artist_id']\n",
    "# columns_to_drop = []\n",
    "\n",
    "# for col in colnames:\n",
    "#     for col_to_drop in colnames_to_drop:\n",
    "#         columns_to_drop.append(str(col)+str(col_to_drop))\n",
    "                    \n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_set.drop(columns_to_drop, axis=1, inplace=True)\n",
    "train_set.drop(['platform_name_0', 'platform_name_1', 'platform_name_2', 'platform_family_0', 'platform_family_1', 'platform_family_2'], axis=1, inplace=True)\n",
    "train_set.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_labels = train_set['is_listened'].as_matrix()\n",
    "train_set.drop(['is_listened'], axis=1, inplace=True)\n",
    "train_set = train_set.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_set, train_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del train_set, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "ipca = IncrementalPCA(n_components=35, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train = ipca.fit_transform(X_train)\n",
    "X_test = ipca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "3 100 begin\n",
    "train_acc:  0.8586797882\n",
    "test_acc: 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print 'da'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "depths = [3,4]\n",
    "n_estimators = [100, 300, 500]\n",
    "for depth in depths:\n",
    "    for n_estimator in n_estimators:\n",
    "        if depth == 3 and n_estimator == 100:\n",
    "            continue\n",
    "        else:\n",
    "            print depth, n_estimator, 'begin'\n",
    "            xgb = XGBClassifier(max_depth = depth, n_estimators=n_estimator)\n",
    "            xgb.fit(X_train, y_train)\n",
    "            predicted = xgb.predict(X_test)\n",
    "            print 'train_acc: ', accuracy_score(y_train, xgb.predict(X_train))\n",
    "            print 'test_acc: ', accuracy_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "depths = [5, 6, 7]\n",
    "n_estimators = [100, 300, 500, 1000]\n",
    "for depth in depths:\n",
    "    for n_estimator in n_estimators:\n",
    "        print depth, n_estimator, 'begin'\n",
    "        xgb = XGBClassifier(max_depth = depth, n_estimators=n_estimator)\n",
    "        xgb.fit(X_train, y_train)\n",
    "        predicted = xgb.predict(X_test)\n",
    "        print 'train_acc: ', accuracy_score(y_train, xgb.predict(X_train))\n",
    "        print 'test_acc: ', accuracy_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "ipca = IncrementalPCA(n_components=35, batch_size=1024)\n",
    "train_set = ipca.fit_transform(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_set = scaler.fit_transform(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(max_depth = 5, n_estimators=1000)\n",
    "xgb.fit(train_set, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_set = scaler.fit_transform(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(max_depth = 4, n_estimators=100)\n",
    "xgb.fit(train_set, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('postprocessed_new_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_set.drop(columns_to_drop, axis=1, inplace=True)\n",
    "test_set.drop(['platform_name_0', 'platform_name_1', 'platform_name_2', 'platform_family_0', 'platform_family_1', 'platform_family_2'], axis=1, inplace=True)\n",
    "test_set.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_set = test_set.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_set = ipca.transform(test_set)\n",
    "test_set = scaler.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predicted = xgb.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_, _, SAMPLE_ID_TEST = io_tools.read_data(train_filename='./data/train.csv',\n",
    "                                                         test_filename='./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_output = pd.DataFrame()\n",
    "df_output['sample_id'] = SAMPLE_ID_TEST\n",
    "df_output['is_listened'] = predicted\n",
    "df_output[['sample_id','is_listened']].to_csv('./predictions_SANYA.csv', sep = \",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(train_labels, xgb.predict(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "amount_of_data = 0.75\n",
    "n = np.shape(train_set)[0]\n",
    "permutation = np.array(range(n))\n",
    "np.random.shuffle(permutation)\n",
    "part_of_data = train_set[:int(n * amount_of_data), :]\n",
    "part_of_labels = train_labels[:int(n * amount_of_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del train_set, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=35)\n",
    "pca.fit(part_of_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# for i in range(np.shape(train_set)[1]):\n",
    "#     train_set[:,i] -= np.mean(train_set[:,i])\n",
    "#     train_set[:,i] /= np.std(train_set[:,i])\n",
    "# from sklearn import preprocessing\n",
    "# train_set = preprocessing.scale(train_set)\n",
    "# np.std(train_set[:,0])\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_set)\n",
    "# part_of_data = scaler.fit_transform(part_of_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(train_labels, xgb.predict(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# del ExtraTreesClassifier, GridSearchCV, SelectFromModel, cross_validation\n",
    "# del x, k, xgb, plt\n",
    "# del XGBClassifier\n",
    "# del getsizeof\n",
    "# del scaler\n",
    "# del train_labels\n",
    "# del analysis_tools\n",
    "# del datasets\n",
    "# del io_tools\n",
    "# del metrics\n",
    "# del preprocessing_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(max_depth = 4, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "part_of_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xgb.fit(train_set, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(part_of_labels, xgb.predict(part_of_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del part_of_data, part_of_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('postprocessed_new_test.csv')\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('postprocessed_new_test.csv')\n",
    "test_set.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_set = test_set.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_set = pca.transform(test_set)\n",
    "test_set = scaler.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del pca, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predicted = xgb.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_, _, SAMPLE_ID_TEST = io_tools.read_data(train_filename='./data/train.csv',\n",
    "                                                         test_filename='./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_output = pd.DataFrame()\n",
    "df_output['sample_id'] = SAMPLE_ID_TEST\n",
    "df_output['is_listened'] = predicted\n",
    "df_output[['sample_id','is_listened']].to_csv('./predictions_LEXA.csv', sep = \",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(train_labels, xgb.predict(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del train_labels\n",
    "del train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print 'lexa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocessed_test_dataset = pd.read_csv('preprocessed_test_set_da.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del train_set\n",
    "del train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=-1,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "xgb1.fit(part_of_data, part_of_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(part_of_labels, xgb1.predict(part_of_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print 'da'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scaler.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=80)\n",
    "pca.fit(train_set)\n",
    "# train_set = pca.fit_transform(train_set)\n",
    "# test_set = pca.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_set)\n",
    "# train_set = \n",
    "# test_set = scaler.transform(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocessed_train_dataset.drop(['context_type', 'release_year'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocessed_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# just_for_fun = preprocessed_train_dataset[]\n",
    "# preprocessed_train_dataset[['release_year', 'is_listened']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocessed_train_dataset = pd.get_dummies(preprocessed_train_dataset, columns=['timestamp_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ['timestamp_hour', 'release_year', 'context_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(preprocessed_train_dataset['timestamp_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# merged_datasets = preprocessing_tools.get_merged_datasets(preprocessed_train_dataset, preprocessed_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "columns_to_check = ['timestamp_hour', 'timestamp_weekday', 'release_year', 'context_type', 'user_age', 'platform_name', 'platform_family', 'user_gender', 'listen_type']\n",
    "for col in columns_to_check:\n",
    "    print col, 'train: ', len(preprocessed_train_dataset[col].unique()), ' test: ', len(preprocessed_test_dataset[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocessed_train_dataset.drop(['genre_id','ts_listen','media_id',\n",
    "                                 'album_id', \n",
    "                                 'release_date',\n",
    "                                 'user_id',\n",
    "                                 'artist_id',\n",
    "                                 'release_date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocessed_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(preprocessed_train_dataset, columns=['listen_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def add_categorical_variable(train, test, variable):\n",
    "    return add_categorical_variables(train, [variable]), add_categorical_variables(test, [variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "add_categorical_variable(preprocessed_train_dataset, preprocessed_test_dataset, 'listen_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preprocessed_train_dataset = add_categorical_variables(preprocessed_train_dataset, ['timestamp_hour', 'timestamp_weekday', 'user_age', 'platform_name', 'platform_family', 'user_gender', 'listen_type'])\n",
    "preprocessed_test_dataset = add_categorical_variables(preprocessed_test_dataset, ['timestamp_hour', 'timestamp_weekday', 'user_age', 'platform_name', 'platform_family', 'user_gender', 'listen_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "merged_datasets = add_categorical_variables(merged_datasets, ['timestamp_hour', 'timestamp_weekday', 'release_year', 'context_type', 'user_age', 'platform_name', 'platform_family', 'user_gender', 'listen_type'])\n",
    "# merged_datasets = merged_datasets.drop(['ts_listen', 'genre_id', 'media_id', 'album_id', 'user_id', 'artist_id', 'release_date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training_size = len(preprocessed_train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_set = (merged_datasets[0:training_size]).as_matrix()\n",
    "test_set = (merged_datasets[training_size:]).as_matrix()\n",
    "train_labels = preprocessed_train_dataset['is_listened'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del preprocessed_train_dataset\n",
    "del preprocessed_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_set = scaler.fit_transform(train_set)\n",
    "test_set = scaler.transform(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=100)\n",
    "train_set = pca.fit_transform(train_set)\n",
    "test_set = pca.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=-1,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "xgb1.fit(train_set, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gbrpred = xgb1.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_output = pd.DataFrame()\n",
    "df_output['sample_id'] = IDtest\n",
    "df_output['is_listened'] = gbrpred\n",
    "df_output[['sample_id','is_listened']].to_csv('./new_output.csv', sep = \",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<h1> III. Feature Engineering </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Genre_id, media_id, album_id, user_id, artist_id -> aggregate (e.g. count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Mean of is_listened by ___ on flow or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-04-17T23:31:23.422Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "merged_datasets = aggregation_functions(merged_datasets, preprocessed_train_dataset, 'user')\n",
    "merged_datasets = aggregation_functions(merged_datasets, preprocessed_train_dataset, 'artist')\n",
    "merged_datasets = aggregation_functions(merged_datasets, preprocessed_train_dataset, 'media')\n",
    "merged_datasets = aggregation_functions(merged_datasets, preprocessed_train_dataset, 'album')\n",
    "merged_datasets = aggregation_functions(merged_datasets, preprocessed_train_dataset, 'genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "merged_datasets['ts_listen']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T22:34:25.619502",
     "start_time": "2017-04-17T22:32:57.290189"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Compute age of song at the moment of listening (still some weird things on very few points...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-04-17T23:31:24.046Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "combined['age_song'] = combined['ts_listen'] - combined['release_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-04-17T23:31:24.526Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Other ideas: compute mean length for an album, an artist, a genre, mean of is_listened for each user, each artist, etc using the date "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Using the date, we can compute the number of songs he listened in a row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Time since previous song (what to do with the first value?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-04-17T23:31:35.091Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "combined_sorted = combined[['ts_listen', 'user_id']].sort_values(['user_id', 'ts_listen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-04-17T23:31:38.933Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "combined_sorted['time_to_prev'] = combined_sorted['user_id']\n",
    "combined_sorted['time_to_next'] = combined_sorted['user_id']\n",
    "\n",
    "# for usr in combined_sorted['user_id'].unique():\n",
    "#     lts = np.array([0] + list(combined_sorted[combined_sorted['user_id'] == usr]['ts_listen']) + [time.time()])\n",
    "#     combined_sorted[combined_sorted['user_id'] == usr]['time_to_prev'] = (lts[1:] - lts[:-1])[:-1]\n",
    "#     combined_sorted[combined_sorted['user_id'] == usr]['time_to_next'] = (lts[1:] - lts[:-1])[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-04-17T23:31:55.712Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "combined = combined.join(combined_sorted[['time_to_prev','time_to_next']], None, 'left', 'l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-04-17T23:31:55.894Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## CENTER DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T14:06:29.060534",
     "start_time": "2017-04-17T14:06:20.267080"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "for _ in combined.columns:\n",
    "    if np.std(combined[_][:len(train)]) != 0:\n",
    "        combined[_] = (combined[_] - np.mean(combined[_][:len(train)]))/np.std(combined[_][:len(train)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<h1> IV. Modeling </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Separate the modified train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T14:07:03.070356",
     "start_time": "2017-04-17T14:06:31.009641"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "combined.drop('index',inplace=True,axis=1)\n",
    "newtrain = combined[:len(train)]\n",
    "newtest = combined[len(train):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## A function to visualize the importance of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T14:07:05.206946",
     "start_time": "2017-04-17T14:07:04.217832"
    },
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def importanceVisualisation(feature_importance, predictors, firstN = 40):\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = [40,10]\n",
    "    # make importances relative to max importance\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(pos[-firstN:], feature_importance[sorted_idx][-firstN:], align='center')\n",
    "    plt.xticks(pos[-firstN:], predictors[sorted_idx][-firstN:], rotation='vertical')\n",
    "    plt.ylabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<h1> IV - 1. Gradient Boosting </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T14:07:15.691463",
     "start_time": "2017-04-17T14:07:15.122319"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "params = {'n_estimators': 1000, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'huber', 'verbose':1}\n",
    "gbr = ensemble.GradientBoostingRegressor(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gbr.fit(train_set, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gbr.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T21:24:58.499391",
     "start_time": "2017-04-17T21:24:46.735822"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "importanceVisualisation(gbr.feature_importances_, newtrain.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<h1> IV - 2. XGBoost </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T02:22:37.245908",
     "start_time": "2017-04-17T02:22:36.708113"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50, plot = False):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=target.values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], target, eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(target.values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(target, dtrain_predprob))\n",
    "    predicted_cv = cross_val_predict(alg, dtrain, target, cv = 5, n_jobs = -1)\n",
    "    print('AUC Score (CV):',metrics.roc_auc_score(target, predicted_cv))\n",
    "    \n",
    "    if plot:\n",
    "        importanceVisualisation(pd.Series(alg.booster().get_fscore()), predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T20:28:35.517307",
     "start_time": "2017-04-16T20:28:35.470792"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "predictors = newtrain.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## FIRST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T17:57:02.387420",
     "start_time": "2017-04-16T11:56:06.143Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=-1,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "xgb1.fit(train_set, train_labels)\n",
    "gbrpred = xgb1.predict(test_set)\n",
    "df_output = pd.DataFrame()\n",
    "df_output['sample_id'] = IDtest\n",
    "df_output['is_listened'] = gbrpred\n",
    "df_output[['sample_id','is_listened']].to_csv('./predictions/GBRoutput.csv', sep = \",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## GRID SEARCH ON XGBOOST TO FIND THE BEST PARAMETERS (VERY LONG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Grid search on 'max_depth' and 'min_child_weight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T17:57:02.387739",
     "start_time": "2017-04-16T11:56:06.162Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# param_test1 = {\n",
    "#  'max_depth':np.array(range(3,10,2)),\n",
    "#  'min_child_weight':np.array(range(1,6,2))\n",
    "# }\n",
    "# gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    "#  min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "#  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    "#  param_grid = param_test1, scoring='roc_auc',n_jobs=-1,iid=False, cv=5)\n",
    "# gsearch1.fit(newtrain[predictors],target)\n",
    "# gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Finer grid search on 'max_depth' and 'min_child_weight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T17:57:02.388042",
     "start_time": "2017-04-16T11:56:06.182Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# param_test2 = {\n",
    "#  'max_depth':[2,3,4],\n",
    "#  'min_child_weight':[2,3,4]\n",
    "# }\n",
    "# gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=5,\n",
    "#  min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "#  objective= 'binary:logistic', nthread=-1, scale_pos_weight=1,seed=27), \n",
    "#  param_grid = param_test2, scoring='roc_auc',n_jobs=-1,iid=False, cv=5)\n",
    "# gsearch2.fit(newtrain[predictors],target)\n",
    "# gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Grid search on 'gamma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T17:57:02.388367",
     "start_time": "2017-04-16T11:56:06.189Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# param_test3 = {\n",
    "#  'gamma':[i/10.0 for i in range(0,5)]\n",
    "# }\n",
    "# gsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=4,\n",
    "#  min_child_weight=4, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "#  objective= 'binary:logistic', nthread=-1, scale_pos_weight=1,seed=27), \n",
    "#  param_grid = param_test3, scoring='roc_auc',n_jobs=-1,iid=False, cv=5)\n",
    "# gsearch3.fit(newtrain[predictors],target)\n",
    "# gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Grid search on 'subsample' and 'colsample_bytree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T17:57:02.388665",
     "start_time": "2017-04-16T11:56:06.195Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# param_test4 = { \n",
    "#  'subsample':[i/10.0 for i in range(6,11)],\n",
    "#  'colsample_bytree':[i/10.0 for i in range(6,11)]\n",
    "# }\n",
    "# gsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=4,\n",
    "#  min_child_weight=4, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "#  objective= 'binary:logistic', nthread=-1, scale_pos_weight=1,seed=27), \n",
    "#  param_grid = param_test4, scoring='roc_auc',n_jobs=-1,iid=False, cv=5)\n",
    "# gsearch4.fit(newtrain[predictors],target)\n",
    "# gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Grid search on 'reg_alpha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T17:57:02.388961",
     "start_time": "2017-04-16T11:56:06.217Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# param_test6 = {\n",
    "#  'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "# }\n",
    "# gsearch6 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=4,\n",
    "#  min_child_weight=4, gamma=0, subsample=1.0, colsample_bytree=0.6,\n",
    "#  objective= 'binary:logistic', nthread=-1, scale_pos_weight=1,seed=27), \n",
    "#  param_grid = param_test6, scoring='roc_auc',n_jobs=-1,iid=False, cv=5)\n",
    "# gsearch6.fit(newtrain,target)\n",
    "# gsearch6.grid_scores_, gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T17:57:02.389261",
     "start_time": "2017-04-16T11:56:06.227Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# param_test7 = {\n",
    "#  'reg_alpha':[0.5,1,2,5,10]\n",
    "# }\n",
    "# gsearch7 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=4,\n",
    "#  min_child_weight=4, gamma=0, subsample=1.0, colsample_bytree=0.6,\n",
    "#  objective= 'binary:logistic', nthread=-1, scale_pos_weight=1,seed=27), \n",
    "#  param_grid = param_test7, scoring='roc_auc',n_jobs=-1,iid=False, cv=5)\n",
    "# gsearch7.fit(newtrain,target)\n",
    "# gsearch7.grid_scores_, gsearch7.best_params_, gsearch7.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Grid search on 'learning_rate' and 'n_estimators'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": true,
     "read_only": true
    }
   },
   "outputs": [],
   "source": [
    "param_test8 = {\n",
    " 'learning_rate': [0.1,0.01,0.001],\n",
    " 'n_estimators': [1000,5000,10000]\n",
    "}\n",
    "gsearch8 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=0, gamma=0.3, subsample=0.9, reg_alpha = 0.2, colsample_bytree=0.9,\n",
    " objective= 'binary:logistic', nthread=-1, scale_pos_weight=1,seed=40), \n",
    " param_grid = param_test8, scoring='roc_auc',n_jobs=-1,iid=False, cv=5)\n",
    "gsearch8.fit(newtrain,target)\n",
    "gsearch8.grid_scores_, gsearch8.best_params_, gsearch8.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## FINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-16T20:28:44.617957",
     "start_time": "2017-04-16T20:28:44.611256"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "init_cell": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# XgbParams = {'learning_rate': 0.001, 'n_estimators':10000, 'max_depth':6, 'min_child_weight':3, 'gamma':0,\n",
    "#             'subsample':0.9, 'colsample_bytree':0.6, 'reg_alpha':1e-5, 'objective': 'binary:logistic',\n",
    "#              'nthread':-1, 'scale_pos_weight':1}\n",
    "\n",
    "XgbParams = {'learning_rate': 0.001, 'n_estimators':1000, 'max_depth':5, 'min_child_weight':0, 'gamma':0.3,\n",
    "            'subsample':0.9, 'colsample_bytree':0.9, 'reg_alpha':0.2, 'objective': 'binary:logistic',\n",
    "             'nthread':-1, 'scale_pos_weight':1}\n",
    "\n",
    "# XgbParams = {'learning_rate': 0.01, 'n_estimators':1000, 'max_depth':4, 'min_child_weight':4, 'gamma':0,\n",
    "#             'subsample':1, 'colsample_bytree':0.6, 'reg_alpha':1, 'objective': 'binary:logistic',\n",
    "#              'nthread':-1, 'scale_pos_weight':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-04-16T18:28:44.776Z"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "xgb3 = XGBClassifier(**XgbParams, seed=27)\n",
    "modelfit(xgb3, newtrain, predictors, plot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hide_input": true,
    "run_control": {
     "frozen": true,
     "read_only": true
    }
   },
   "outputs": [],
   "source": [
    "xgb3 = XGBClassifier(**XgbParams, seed=40)\n",
    "modelfit(xgb3, newtrain, predictors, plot = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<h1> VI. Final Prediction </h1> (We take the best xgboost model here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-17T21:25:01.810949",
     "start_time": "2017-04-17T21:24:58.518556"
    },
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "gbrpred = gbr.predict(newtest)\n",
    "df_output = pd.DataFrame()\n",
    "df_output['sample_id'] = IDtest\n",
    "df_output['is_listened'] = gbrpred\n",
    "df_output[['sample_id','is_listened']].to_csv('./predictions/GBRoutput.csv', sep = \",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
